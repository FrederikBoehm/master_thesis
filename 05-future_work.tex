\chapter{Future Work}
\label{chap:future_work}
As we discovered, the ability of our approach to preserve the look of the surface mesh is limited by the shading of the volumes.
The next step to improve on this would be to include anisotropic extinction, which would require a rewrite of the filtering procedure and the distance sampling and transmittance estimation.
The existing volumes would not be compatible with this new approach and had to be re-generated as well.

To further enhance the quality of the renderings, it would be interesting how our volumetric \ac{lod} approach performs with spectral rendering.
This would improve physical accuracy since the scattering in media is actually wavelength dependent \cite{novak_overview}, but it requires a rewrite of the renderer.
Since this increases the dimensionality of the integration domain it leads to longer render times and is currently only feasible for offline rendering.
We should still be able to achieve an improvement of the render durations.

Additionally, we could improve the quality of our volumetric representations.
Currently, we use 100 rays in each voxel to filter a mesh.
This is an appropriate number for small voxel sizes like $\SI{0.1}{\m}$ or $\SI{0.2}{\m}$.
For large voxel sizes, like $\SI{3.2}{\m}$ or $\SI{6.4}{\m}$, this can lead to an undersampling, since the rays are further apart.
Therefore, it would be worthwhile to make the number of rays dependent on the volume of a voxel, to ensure a constant sampling density across all voxel sizes.
It is difficult to predict the magnitude of this quality improvement, especially because the largest difference can be expected for the furthest \acsp{lod}.

Another interesting area of research would be to further improve the performance of our approach and possibly switch the \acsp{lod} on the fly.
This would allow a camera movement without making the \acsp{lod} misalign the current view frustum.
The scene generator would have to be written in C++ for that and optimization of the program itself have to be made.
A substantial improvement can be expected by caching the free positions in the forest area, since we initialize the random number generator with the same seed anyway.
However, this is limited to our forest scene.
A more portable optimization would be to approximate the pixels a bounding box covers by a rectangle instead of the convex hull that we currently use.
We could further move the distance sampling from the ray generation shader to the intersection shader.
This would most probably improve performance, since the \ac{bvh} is not traversed a second time in case of a null-collision.
Also, using brick grid for storing all voxel attributes like color values or the \ac{sggx} matrix $S$ might be worth investigating regarding the performance implications.

Having the knowledge that rough \acsp{lod} do not necessarily render faster than meshes, a further optimization would be to not simply set models outside of the view frustum to the roughest \ac{lod}.
Instead, it might be beneficial to also incorporate the distance to the camera for these and use the mesh representations first.
Only after a certain distance from the camera the roughest \ac{lod} would be selected.
Furthermore, we might want to use a different heuristic of \ac{lod} selection for each model, since we learned from Figure \ref{fig:render_time_comparisons} that \ac{lod} rendering surpasses mesh rendering at different distances for different models.